<p align = "center" draggable=‚Äùfalse‚Äù ><img src="https://github.com/AI-Maker-Space/LLM-Dev-101/assets/37101144/d1343317-fa2f-41e1-8af1-1dbb18399719" 
     width="200px"
     height="auto"/>
</p>

## <h1 align="center" id="heading">Session 14: Build & Serve Agentic Graphs with LangGraph</h1>

| ü§ì Pre-work | üì∞ Session Sheet | ‚è∫Ô∏è Recording     | üñºÔ∏è Slides        | üë®‚Äçüíª Repo         | üìù Homework      | üìÅ Feedback       |
|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|:-----------------|
| [Session 14: Pre-Work](https://www.notion.so/Session-14-Deploying-Agents-to-Production-21dcd547af3d80aba092fcb6c649c150?source=copy_link#247cd547af3d80709683ff380f4cba62)| [Session 14: Deploying Agents to Production](https://www.notion.so/Session-14-Deploying-Agents-to-Production-21dcd547af3d80aba092fcb6c649c150) | [Recording!](https://us02web.zoom.us/rec/share/1YepNUK3kqQnYLY8InMfHv84JeiOMyjMRWOZQ9jfjY86dDPvHMhyoz5Zo04w_tn-.91KwoSPyP6K6u0DC)  (@@5J6DVQ)| [Session 14 Slides](https://www.canva.com/design/DAGvVPg7-mw/IRwoSgDXPEqU-PKeIw8zLg/edit?utm_content=DAGvVPg7-mw&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton) | You are here! | [Session 14 Assignment: Production Agents](https://forms.gle/nZ7ugE4W9VsC1zXE8) | [AIE7 Feedback 8/7](https://forms.gle/juo8SF5y5XiojFyC9)

# Build üèóÔ∏è

Run the repository and complete the following:

- ü§ù Breakout Room Part #1 ‚Äî Building and serving your LangGraph Agent Graph
  - Task 1: Getting Dependencies & Environment
    - Configure `.env` (OpenAI, Tavily, optional LangSmith)
  - Task 2: Serve the Graph Locally
    - `uv run langgraph dev` (API on http://localhost:2024)
  - Task 3: Call the API
    - `uv run test_served_graph.py` (sync SDK example)
  - Task 4: Explore assistants (from `langgraph.json`)
    - `agent` ‚Üí `simple_agent` (tool-using agent)
    - `agent_helpful` ‚Üí `agent_with_helpfulness` (separate helpfulness node)

- ü§ù Breakout Room Part #2 ‚Äî Using LangGraph Studio to visualize the graph
  - Task 1: Open Studio while the server is running
    - https://smith.langchain.com/studio?baseUrl=http://localhost:2024
  - Task 2: Visualize & Stream
    - Start a run and observe node-by-node updates
  - Task 3: Compare Flows
    - Contrast `agent` vs `agent_helpful` (tool calls vs helpfulness decision)

<details>
<summary>üöß Advanced Build üöß (OPTIONAL - <i>open this section for the requirements</i>)</summary>

- Create and deploy a locally hosted MCP server with FastMCP.
- Extend your tools in `tools.py` to allow your LangGraph to consume the MCP Server.
</details>

# Ship üö¢

- Running local server (`langgraph dev`)
- Short demo showing both assistants responding

# Share üöÄ
- Walk through your graph in Studio
- Share 3 lessons learned and 3 lessons not learned


#### ‚ùì Question:

What is the purpose of the `chunk_overlap` parameter when using `RecursiveCharacterTextSplitter` to prepare documents for RAG, and what trade-offs arise as you increase or decrease its value?

##### ‚úÖ Answer:

1. 'chunk_overlap' defines how big will be an overlap between documents when they are split. It helps create continuity between documents, e.g., when we split one document in two, and without chunk_overlap only one documen may retain a piece of information that is contextually relevant to both chunks.
2. When we increase chunk_overlap's value too much, we create chunks that are a) way too similar b) don't have enough unique context. In addition, we'll start creating too many chunks.
3. When we decrease chunk_overlap's value too much, we create a potential issue with documents not having relevant context because only the previous chunk contains it, but the piece of information is relevan to both previous chunk and this chunk. 


#### ‚ùì Question:

Your retriever is configured with `search_kwargs={"k": 5}`. How would adjusting `k` likely affect RAGAS metrics such as Context Precision and Context Recall in practice, and why?

##### ‚úÖ Answer:
1. Context Precision and Context Recall are somewhat at odds with each other, metaphorically speaking. They will react in different directions.
2. Increasing 'k' will decrease precision and increase recall. With more documents retrieved, the number of relevant documents retrieved out of all documents retrieved (precision) will become lower because we'll start pulling less and less relevant documents. However, the completeness of context (recall) from retrieved documents will increase, because with more documents we have a higher likelihood of covering all the bases.
3. Decreasing 'k' will increase precision and decrease recall. Less documents means that we increase the chance of all documents being relevant (good for precision), but completeness may take a hit because we'll have overall less context (bad for recall).

#### ‚ùì Question:

Compare the `agent` and `agent_helpful` assistants defined in `langgraph.json`. Where does the helpfulness evaluator fit in the graph, and under what condition should execution route back to the agent vs. terminate?

##### ‚úÖ Answer:
1. Helpful agent decides whether to pass to helpfulness evaluator based on whether the last "message" was "action" or not. If it was an "action", then it passes the turn to helpfuless evaluator.
2. Helpful agent terminates when: a) it exceeds 10 messages b)  when response is deemed helpful.
3. Simple agent simply runs a tool and provides an answer. Simple agent, compared to the helpful agent, doesn't check for helpfulness. 