{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\" draggable=â€falseâ€ ><img src=\"https://github.com/AI-Maker-Space/LLM-Dev-101/assets/37101144/d1343317-fa2f-41e1-8af1-1dbb18399719\" \n",
    "     width=\"200px\"\n",
    "     height=\"auto\"/>\n",
    "</p>\n",
    "\n",
    "<h1 align=\"center\" id=\"heading\">OpenAI Agents SDK - AIM</h1>\n",
    "\n",
    "In this notebook, we'll go over some of the key features of the OpenAI Agents SDK - as explored through a notebook-ified version of their [Research Bot](https://github.com/openai/openai-agents-python/tree/main/examples/research_bot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You don't need to run this cell if you're running this notebook locally. \n",
    "\n",
    "#!pip install -qU openai-agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nest Async:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "As may be expected, the primary thing we'll do in the Agents SDK is construct Agents!\n",
    "\n",
    "Agents are constructed with a few basic properties:\n",
    "\n",
    "- A prompt, which OpenAI is using the language \"instruction\" for, that determines the behaviour or goal of the Agent\n",
    "- A model, the \"brain\" of the Agent\n",
    "\n",
    "They also typically include an additional property: \n",
    "\n",
    "- Tool(s) that equip the Agent with things it can use to get stuff done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Create Planner Agent\n",
    "\n",
    "Let's start by creating our \"Planner Agent\" - which will come up with the initial set of search terms that should answer a query provided by the user. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from agents import Agent\n",
    "\n",
    "PLANNER_PROMPT = (\n",
    "    \"You are a helpful research assistant. Given a query, come up with a set of web searches to perform\" \n",
    "    \"to best answer the query. Output between 5 and 20 terms to query for.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the data models that our Planner Agent will use to structure its output. We'll create:\n",
    "\n",
    "1. `WebSearchItem` - A model for individual search items, containing the search query and reasoning\n",
    "2. `WebSearchPlan` - A container model that holds a list of search items\n",
    "\n",
    "These Pydantic models will help ensure our agent returns structured data that we can easily process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebSearchItem(BaseModel):\n",
    "    reason: str\n",
    "    \"Your reasoning for why this search is important to the query.\"\n",
    "\n",
    "    query: str\n",
    "    \"The search term to use for the web search.\"\n",
    "\n",
    "class WebSearchPlan(BaseModel):\n",
    "    searches: list[WebSearchItem]\n",
    "    \"\"\"A list of web searches to perform to best answer the query.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create our Planner Agent using the Agent class from the OpenAI Agents SDK. This agent will use the instructions defined in `PLANNER_PROMPT` and will output structured data in the form of our WebSearchPlan model. We're using the GPT-4o model for this agent to ensure high-quality search term generation.\n",
    "\n",
    "> NOTE: When we provide an `output_type` - the model will return a [structured response](https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner_agent = Agent(\n",
    "    name=\"PlannerAgent\",\n",
    "    instructions=PLANNER_PROMPT,\n",
    "    model=\"gpt-4.1\",\n",
    "    output_type=WebSearchPlan,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“Question #1:\n",
    "\n",
    "Why is it important to provide a structured response template? (As in: Why are structured outputs helpful/preferred in Agentic workflows?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### âœ… Answer:\n",
    "1. Structured output validates the provided response, and ensures it comes in a way that we expect. LLM output, especially Agentic output, can be used in a variety of ways by other agents or later parts of the pipeline. \n",
    "2. In the provided example, we want our agent to provide us with a list of search queries. If we use that list in the pipeline to do search queries, we need to ensure we can a) differentiate between queries (where one finishes, and another starts, otherwise we may interpret it as a single query) and b) avoid writing hard-to-maintain parsing functions E.g., we can simply prompt the model to provide us a with search queries separating them by new line, write a custom function to parse it to a list, and then continue working with this as a list. But why do it? It creates a need to maintain a custom function, doesn't guarantee we'll get expected output for whatever reason (which is more often with simple models), and introduces overheads for engineers to understand how system works. Instead, we simply pass expected type of output next in the pipeline.\n",
    "\n",
    "Additional example: in a more complex solutions it becomes critical. I have an AI Public Speaking Coach, which provides a multi-faceted evaluation of your response, including a variety of scores, etc.. While I can simply prompt to provide a json output, I'm at a mercy of LLM to do that, while also needing to parse that output myself and write multiple validating functions. It makes the code hard to maintain and unnecessarily complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Create Search Agent\n",
    "\n",
    "Now we'll create our Search Agent, which will be responsible for executing web searches based on the terms generated by the Planner Agent. This agent will take each search query, perform a web search using the `WebSearchTool`, and then summarize the results in a concise format.\n",
    "\n",
    "> NOTE: We are using the `WebSearchTool`, a hosted tool that can be used as part of an `OpenAIResponsesModel` as outlined in the [documentation](https://openai.github.io/openai-agents-python/tools/). This is based on the tools available through OpenAI's new [Responses API](https://openai.com/index/new-tools-for-building-agents/).\n",
    "\n",
    "The `SEARCH_PROMPT` below instructs the agent to create brief, focused summaries of search results. These summaries are designed to be 2-3 paragraphs, under 300 words, and capture only the essential information without unnecessary details. The goal is to provide the Writer Agent with clear, distilled information that can be efficiently synthesized into the final report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_PROMPT = (\n",
    "    \"You are a research assistant. Given a search term, you search the web for that term and\"\n",
    "    \"produce a concise summary of the results. The summary must 2-3 paragraphs and less than 300\"\n",
    "    \"words. Capture the main points. Write succinctly, no need to have complete sentences or good\"\n",
    "    \"grammar. This will be consumed by someone synthesizing a report, so its vital you capture the\"\n",
    "    \"essence and ignore any fluff. Do not include any additional commentary other than the summary\"\n",
    "    \"itself.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create our Search Agent using the Agent class from the OpenAI Agents SDK. This agent will use the instructions defined in `SEARCH_PROMPT` and will utilize the `WebSearchTool` to perform web searches. We're configuring it with `tool_choice=\"required\"` to ensure it always uses the search tool when processing requests.\n",
    "\n",
    "> NOTE: We can, as demonstrated, indicate how we want our model to use tools. You can read more about that at the bottom of the page [here](https://openai.github.io/openai-agents-python/agents/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import WebSearchTool\n",
    "from agents.model_settings import ModelSettings\n",
    "\n",
    "search_agent = Agent(\n",
    "    name=\"Search agent\",\n",
    "    instructions=SEARCH_PROMPT,\n",
    "    tools=[WebSearchTool()],\n",
    "    model_settings=ModelSettings(tool_choice=\"required\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“ Question #2: \n",
    "\n",
    "What other tools are supported in OpenAI's Responses API?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### âœ… Answer:\n",
    "1. Function â€” LLM can call any function that we define in our code.\n",
    "2. File Search - out-of-the-box document retrieval for RAG use cases.\n",
    "3. Web Search - perform web search.\n",
    "4. Computer use - LLM emulates computer usage in a virtual environment. Can perform actions through UI when API/MCP/functions are not available.\n",
    "5. MCP tool - accesses tools defined by a specific MCP server.\n",
    "6. Code interpreter - writes and executes code. Very effective replacement for function calling when the use case calls for multiple function calls (when functions do not support batch operations), as the LLM can write a loop.\n",
    "7. Image generation - generates an image.\n",
    "8. Local shell - executes shell commans in local environment. Needed for CLI use cases, e.g., even simply listing files in the directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Create Writer Agent\n",
    "\n",
    "Finally, we'll create our Writer Agent, which will synthesize all the research findings into a comprehensive report. This agent takes the original query and the research summaries from the Search Agent, then produces a structured report with follow-up questions.\n",
    "\n",
    "The Writer Agent will:\n",
    "1. Create an outline for the report structure\n",
    "2. Generate a detailed markdown report (5-10 pages)\n",
    "3. Provide follow-up questions for further research\n",
    "\n",
    "We'll define the prompt for this agent in the next cell. This prompt will instruct the Writer Agent on how to synthesize research findings into a comprehensive report with follow-up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITER_PROMPT = (\n",
    "    \"\"\"You are a senior machine-learning researcher specializing in cutting-edge AI, large-language-model (LLM), and data-science methodologies.\n",
    "Your task is to deliver a cohesive, practitioner-ready report that answers a specified technical research question.\n",
    "\n",
    "Workflow\n",
    "Outline first.\n",
    "\n",
    "You will receive the research question plus any preliminary notes assembled by a junior assistant.\n",
    "\n",
    "Produce a detailed outline showing the logical structure of the eventual report (major headings, sub-headings, key points, and analyses you intend to cover).\n",
    "\n",
    "Full report.\n",
    "\n",
    "Expand the outline into a comprehensive report â‰¥ 1 000 words (â‰ˆ 5â€“10 pages).\n",
    "\n",
    "Write in clear, precise prose suitable for ML engineers, research scientists, and technical executives.\n",
    "\n",
    "Use formal Markdown (#/## headings, code blocks, bullet lists, tables if essential).\n",
    "\n",
    "Follow-up questions.\n",
    "\n",
    "Conclude with exactly five distinct follow-up research questions that would deepen or extend the analysis. Do not repeat questions.\n",
    "\n",
    "Content Requirements\n",
    "Length. Minimum 1 000 words.\n",
    "\n",
    "Technical scope. Address the research question across the ML/AI/LLM/DS stack, which may include:\n",
    "\n",
    "Algorithms & Architectures: recent advances (transformers, diffusion, retrieval-augmented generation, graph nets, etc.).\n",
    "\n",
    "Datasets & Data Engineering: sourcing, curation, labeling, augmentation, synthetic data, privacy considerations.\n",
    "\n",
    "Training & Optimization: objective functions, regularization, hyper-parameter tuning, hardware acceleration.\n",
    "\n",
    "Evaluation & Benchmarking: task-specific metrics, robustness, fairness, interpretability, ablation studies.\n",
    "\n",
    "Deployment & MLOps: model serving, CI/CD, monitoring, rollback, drift detection, cost/performance trade-offs.\n",
    "\n",
    "Responsible AI & Compliance: bias mitigation, fairness audits, privacy guarantees (DP, federated learning), emerging regulations (EU AI Act, U.S. algorithmic accountability bills).\n",
    "\n",
    "Best-practice focus. Identify practical implementation tips, common pitfalls, risk factors, and state-of-the-art baselines.\n",
    "\n",
    "Comparative analysis. Where relevant, compare at least three leading approaches, frameworks, or toolchains (e.g., PyTorch vs. JAX vs. TensorFlow; Hugging Face Transformers vs. OpenAI APIs vs. bespoke in-house stacks). Discuss trade-offs in accuracy, scalability, maintainability, and cost.\n",
    "\n",
    "Evidence hierarchy.\n",
    "\n",
    "Primary sources: peer-reviewed papers, arXiv preprints, major conference proceedings (NeurIPS, ICML, ACL, CVPR, KDD), official framework docs, reproducibility artifacts.\n",
    "\n",
    "Secondary commentary: survey papers, authoritative blog posts, industry white papers, standards drafts (ISO/IEC, NIST).\n",
    "\n",
    "Citations. Use inline references in the format (Author et al., Year) or (Tool Docs, version, Year). Provide URLs for open-access resources when available.\n",
    "\n",
    "Code snippets. Include concise, runnable code blocks only when they materially clarify an algorithm or implementation detail.\n",
    "\n",
    "No consulting or legal advice. Insert a brief disclaimer stating the report is for informational purposes and does not constitute professional, financial, or legal advice.\n",
    "\n",
    "Deliverables\n",
    "Return one Markdown document containing:\n",
    "\n",
    "Outline\n",
    "\n",
    "Full Report\n",
    "\n",
    "\n",
    "Be thorough, analytic, and brutally honestâ€”this document should equip technical leaders and research practitioners to understand, evaluate, and act on the topic at an expert level.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ—ï¸ Activity #1: \n",
    "\n",
    "This prompt is quite generic - modify this prompt to produce a report that is more personalized to either your personal preference, or more appropriate for a specific use case (eg. law domain research)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### âœ… Answer:\n",
    "Done (above). It is now a writer focused on the Legal/Compliance research in the US behavioural health space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create our Writer Agent using the Agent class from the OpenAI Agents SDK. This agent will synthesize all the research findings into a comprehensive report. We're configuring it with the `ReportData` output type to structure the response with a short summary, markdown report, and follow-up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportData(BaseModel):\n",
    "    short_summary: str\n",
    "    \"\"\"A short 2-3 sentence summary of the findings.\"\"\"\n",
    "\n",
    "    markdown_report: str\n",
    "    \"\"\"The final report\"\"\"\n",
    "\n",
    "    follow_up_questions: list[str]\n",
    "    \"\"\"Suggested topics to research further\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define our Writer Agent using the Agent class from the OpenAI Agents SDK. This agent will take the original query and research summaries, then synthesize them into a comprehensive report with follow-up questions. We've defined a custom output type called `ReportData` that structures the response with a short summary, markdown report, and follow-up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_agent = Agent(\n",
    "    name=\"WriterAgent\",\n",
    "    instructions=WRITER_PROMPT,\n",
    "    model=\"o3-mini\",\n",
    "    output_type=ReportData,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“ Question #3: \n",
    "\n",
    "Why are we electing to use a reasoning model for writing our report?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### âœ… Answer:\n",
    "1. In a sense, it emulates an agentic loop of plan->write->verify->rewrite without the need to implement an agentic crew focused on each specific action. It would be less effective than the agentic crew, as each agent could be focused on their task with their specific context, but it still provides a simple way to deepen the report expertise beyond straightforward generation. Reasoning cycles introduce new content in the context and allow self-correction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Create Utility Classes \n",
    "\n",
    "We'll define utility classes to help with displaying progress and managing the research workflow. The Printer class below will provide real-time updates on the research process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Printer class provides real-time progress updates during the research process. It uses Rich's Live display to show dynamic content with spinners for in-progress items and checkmarks for completed tasks. The class maintains a dictionary of items with their completion status and can selectively hide checkmarks for specific items. This creates a clean, interactive console experience that keeps the user informed about the current state of the research workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from rich.console import Console, Group\n",
    "from rich.live import Live\n",
    "from rich.spinner import Spinner\n",
    "\n",
    "class Printer:\n",
    "    def __init__(self, console: Console):\n",
    "        self.live = Live(console=console)\n",
    "        self.items: dict[str, tuple[str, bool]] = {}\n",
    "        self.hide_done_ids: set[str] = set()\n",
    "        self.live.start()\n",
    "\n",
    "    def end(self) -> None:\n",
    "        self.live.stop()\n",
    "\n",
    "    def hide_done_checkmark(self, item_id: str) -> None:\n",
    "        self.hide_done_ids.add(item_id)\n",
    "\n",
    "    def update_item(\n",
    "        self, item_id: str, content: str, is_done: bool = False, hide_checkmark: bool = False\n",
    "    ) -> None:\n",
    "        self.items[item_id] = (content, is_done)\n",
    "        if hide_checkmark:\n",
    "            self.hide_done_ids.add(item_id)\n",
    "        self.flush()\n",
    "\n",
    "    def mark_item_done(self, item_id: str) -> None:\n",
    "        self.items[item_id] = (self.items[item_id][0], True)\n",
    "        self.flush()\n",
    "\n",
    "    def flush(self) -> None:\n",
    "        renderables: list[Any] = []\n",
    "        for item_id, (content, is_done) in self.items.items():\n",
    "            if is_done:\n",
    "                prefix = \"âœ… \" if item_id not in self.hide_done_ids else \"\"\n",
    "                renderables.append(prefix + content)\n",
    "            else:\n",
    "                renderables.append(Spinner(\"dots\", text=content))\n",
    "        self.live.update(Group(*renderables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a ResearchManager class that will orchestrate the research process. This class will:\n",
    "1. Plan searches based on the query\n",
    "2. Perform those searches to gather information\n",
    "3. Write a comprehensive report based on the gathered information\n",
    "4. Display progress using our Printer class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "from agents import Runner, custom_span, gen_trace_id, trace\n",
    "\n",
    "class ResearchManager:\n",
    "    def __init__(self):\n",
    "        self.console = Console()\n",
    "        self.printer = Printer(self.console)\n",
    "\n",
    "    async def run(self, query: str) -> None:\n",
    "        trace_id = gen_trace_id()\n",
    "        with trace(\"Research trace\", trace_id=trace_id):\n",
    "            self.printer.update_item(\n",
    "                \"trace_id\",\n",
    "                f\"View trace: https://platform.openai.com/traces/trace?trace_id={trace_id}\",\n",
    "                is_done=True,\n",
    "                hide_checkmark=True,\n",
    "            )\n",
    "\n",
    "            self.printer.update_item(\n",
    "                \"starting\",\n",
    "                \"Starting research...\",\n",
    "                is_done=True,\n",
    "                hide_checkmark=True,\n",
    "            )\n",
    "            search_plan = await self._plan_searches(query)\n",
    "            search_results = await self._perform_searches(search_plan)\n",
    "            report = await self._write_report(query, search_results)\n",
    "\n",
    "            final_report = f\"Report summary\\n\\n{report.short_summary}\"\n",
    "            self.printer.update_item(\"final_report\", final_report, is_done=True)\n",
    "\n",
    "            self.printer.end()\n",
    "\n",
    "        print(\"\\n\\n=====REPORT=====\\n\\n\")\n",
    "        print(f\"Report: {report.markdown_report}\")\n",
    "        print(\"\\n\\n=====FOLLOW UP QUESTIONS=====\\n\\n\")\n",
    "        unique_questions = []\n",
    "        seen = set()\n",
    "        \n",
    "        for question in report.follow_up_questions:\n",
    "            if question not in seen:\n",
    "                unique_questions.append(question)\n",
    "                seen.add(question)\n",
    "        \n",
    "        for i, question in enumerate(unique_questions, 1):\n",
    "            print(f\"{i}. {question}\")\n",
    "\n",
    "    async def _plan_searches(self, query: str) -> WebSearchPlan:\n",
    "        self.printer.update_item(\"planning\", \"Planning searches...\")\n",
    "        result = await Runner.run(\n",
    "            planner_agent,\n",
    "            f\"Query: {query}\",\n",
    "        )\n",
    "        self.printer.update_item(\n",
    "            \"planning\",\n",
    "            f\"Will perform {len(result.final_output.searches)} searches\",\n",
    "            is_done=True,\n",
    "        )\n",
    "        return result.final_output_as(WebSearchPlan)\n",
    "\n",
    "    async def _perform_searches(self, search_plan: WebSearchPlan) -> list[str]:\n",
    "        with custom_span(\"Search the web\"):\n",
    "            self.printer.update_item(\"searching\", \"Searching...\")\n",
    "            num_completed = 0\n",
    "            max_concurrent = 5\n",
    "            results = []\n",
    "            \n",
    "            for i in range(0, len(search_plan.searches), max_concurrent):\n",
    "                batch = search_plan.searches[i:i+max_concurrent]\n",
    "                tasks = [asyncio.create_task(self._search(item)) for item in batch]\n",
    "                \n",
    "                for task in asyncio.as_completed(tasks):\n",
    "                    try:\n",
    "                        result = await task\n",
    "                        if result is not None:\n",
    "                            results.append(result)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Search error: {e}\")\n",
    "                        \n",
    "                    num_completed += 1\n",
    "                    self.printer.update_item(\n",
    "                        \"searching\", f\"Searching... {num_completed}/{len(search_plan.searches)} completed\"\n",
    "                    )\n",
    "            \n",
    "            self.printer.mark_item_done(\"searching\")\n",
    "            return results\n",
    "\n",
    "    async def _search(self, item: WebSearchItem) -> str | None:\n",
    "        input = f\"Search term: {item.query}\\nReason for searching: {item.reason}\"\n",
    "        try:\n",
    "            result = await Runner.run(\n",
    "                search_agent,\n",
    "                input,\n",
    "            )\n",
    "            return str(result.final_output)\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching for '{item.query}': {e}\")\n",
    "            return None\n",
    "\n",
    "    async def _write_report(self, query: str, search_results: list[str]) -> ReportData:\n",
    "        self.printer.update_item(\"writing\", \"Thinking about report...\")\n",
    "        input = f\"Original query: {query}\\nSummarized search results: {search_results}\"\n",
    "        \n",
    "        result = Runner.run_streamed(\n",
    "            writer_agent,\n",
    "            input,\n",
    "        )\n",
    "        \n",
    "        update_messages = [\n",
    "            \"Thinking about report...\",\n",
    "            \"Planning report structure...\",\n",
    "            \"Writing outline...\",\n",
    "            \"Creating sections...\",\n",
    "            \"Cleaning up formatting...\",\n",
    "            \"Finalizing report...\",\n",
    "            \"Finishing report...\",\n",
    "        ]\n",
    "\n",
    "        last_update = time.time()\n",
    "        next_message = 0\n",
    "        \n",
    "        async for event in result.stream_events():\n",
    "            if time.time() - last_update > 5 and next_message < len(update_messages):\n",
    "                self.printer.update_item(\"writing\", update_messages[next_message])\n",
    "                next_message += 1\n",
    "                last_update = time.time()\n",
    "\n",
    "        self.printer.mark_item_done(\"writing\")\n",
    "        return result.final_output_as(ReportData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ—ï¸ Activity #2:\n",
    "\n",
    "Convert the above flow into a flowchart style image (software of your choosing, but if you're not sure which to use try [Excallidraw](https://excalidraw.com/)) that outlines how the different Agents interact with each other. \n",
    "\n",
    "> HINT: Cursor's AI (CMD+L or CTRL+L on Windows) would be a helpful way to get a basic diagram that you can add more detail to!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### âœ… Answer:\n",
    "![flow](flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Running Our Agent\n",
    "\n",
    "Now let's run our agent! The main function below will prompt the user for a research topic, then pass that query to our ResearchManager to handle the entire research process. The ResearchManager will: \n",
    "\n",
    "1. Break down the query into search items\n",
    "2. Search for information on each item\n",
    "3. Write a comprehensive report based on the search results\n",
    "\n",
    "Let's see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main() -> None:\n",
    "    query = input(\"What would you like to research? \")\n",
    "    await ResearchManager().run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a982c6c5a047caa892b790c30a5f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=====REPORT=====\n",
      "\n",
      "\n",
      "Report: # Evaluating the Two-Step Fine-Tuning Approach for Rule-Based Comment Classification\n",
      "\n",
      "## 1. Introduction\n",
      "\n",
      "Rule-based comment classification aims to determine whether a comment violates a given rule. In practical scenarios, training data may provide explicit examples for only a subset of rules, while the test set may incorporate undisclosed or hidden rules. Such mismatches compel models to generalize beyond the explicit patterns provided during training. This report evaluates a two-stage approach using DeBERTa-v3-large: initially fine-tuning with triplet loss followed by standard classification fine-tuning. We describe what happens during this workflow, assess its effectiveness, identify pitfalls, and suggest possible improvements.\n",
      "\n",
      "## 2. Methodology and Workflow\n",
      "\n",
      "### 2.1 DeBERTa-v3-large Overview\n",
      "\n",
      "DeBERTa-v3-large is a state-of-the-art transformer-based language model that builds on its predecessor with enhancements such as ELECTRA-style pre-training and gradient-disentangled embedding sharing. With 24 layers and 304 million parameters, its architecture is designed to capture fine-grained semantic features, allowing robust performance across various NLP tasks (Microsoft, 2021). Its strong baseline performance on benchmarks like GLUE and CommonsenseQA makes it a logical choice for comment classification tasks.\n",
      "\n",
      "### 2.2 Stage One: Triplet Loss Fine-Tuning\n",
      "\n",
      "The first stage involves fine-tuning DeBERTa-v3-large using triplet loss. The triplet setup requires three samples:\n",
      "\n",
      "- **Anchor (Comment)**: the real user comment under analysis.\n",
      "- **Positive Example**: a comment that is similar in context or compliance with the given rule.\n",
      "- **Negative Example**: a comment that violates the rule or is dissimilar to the anchor.\n",
      "\n",
      "Triplet loss enforces that the distance (in the learned embedding space) between the anchor and the positive sample is smaller than that between the anchor and the negative sample by at least a set margin. This training mechanism encourages the model to learn discriminative features, mapping compliant comments closer together and distancing violative ones.\n",
      "\n",
      "**Key Benefits:**\n",
      "\n",
      "- **Discriminative Representation:** The learned embedding space is structured to reflect semantic similarities and differences.\n",
      "- **Handling Limited Labeled Data:** Triplet loss is effective in few-shot learning settings, which is valuable when positive and negative examples are limited.\n",
      "\n",
      "**Challenges:**\n",
      "\n",
      "- **Triplet Mining:** Selecting informative triplets is crucial. Poor triplet selection might result in embeddings that do not reflect the intricacies of the violation patterns.\n",
      "- **Computational Overhead:** Mining hard triplets and computing pairwise distances increases computational complexity.\n",
      "\n",
      "### 2.3 Stage Two: Rule-Based Classification Fine-Tuning\n",
      "\n",
      "Following the triplet loss phase, the model undergoes an additional fine-tuning phase for classification. The classification task uses a format that includes:\n",
      "\n",
      "- **RULE:** The rule description or trigger condition.\n",
      "- **POSITIVE EXAMPLE:** An exemplar that satisfies the rule's constraints.\n",
      "- **NEGATIVE EXAMPLE:** An exemplar that violates the rule.\n",
      "- **DECISION:** The final prediction indicating whether the comment violates the rule.\n",
      "\n",
      "This stage is conceptually similar to traditional supervised fine-tuning, but it includes structured inputs (rule and examples) as contextual adjuncts. Using the embeddings refined during the triplet training phase, the model is expected to fine-tune its decision boundary in accordance with the rule-based aspects of the classification task.\n",
      "\n",
      "**Advantages:**\n",
      "\n",
      "- **Contextualization:** The explicit inclusion of rule text and exemplar references helps guide the model to relate the learned embeddings with specific rule semantics.\n",
      "- **Rule Generalization:** If trained well, the model may extrapolate learned patterns to unseen rules.\n",
      "\n",
      "**Potential Pitfalls:**\n",
      "\n",
      "- **Catastrophic Forgetting:** The sequential fine-tuning may override some of the benefits yielded by the initial triplet loss stage, especially if the latter phase does not maintain the embedding distribution.\n",
      "- **Generalization to Hidden Rules:** Fine-tuning on a limited rule set might not adequately cover the variability of unseen or more complex rules.\n",
      "- **Data Scarcity and Overfitting:** Limited examples per rule (both in positive and negative instances) can lead to overfitting, where the model may learn rule-specific artifacts rather than generalizable features.\n",
      "\n",
      "## 3. In-Depth Analysis of the Workflow\n",
      "\n",
      "### 3.1 What Happens During Training:\n",
      "\n",
      "1. **Embedding Space Structuring (Triplet Training):**\n",
      "   - The model first learns to differentiate between compliant and non-compliant comments by adjusting embeddings. With triplet loss, the modelâ€™s focus is on reducing the gap between similar samples while increasing the gap against dissimilar ones.\n",
      "   \n",
      "2. **Context Incorporation (Classification Fine-Tuning):**\n",
      "   - The integration of rule, positive example, and negative example into the training input allows the model to anchor its decisions around semantic cues that are rule-specific. This step ideally refines the modelâ€™s ability to map rule descriptions onto the embedding space.\n",
      "   \n",
      "3. **Learning Decision Boundaries:**\n",
      "   - The final classification head is fine-tuned to output binary decisions based on the structured contextual input. The earlier triplet training ideally improves the robustness of these boundaries by ensuring that the underlying representation is semantically rich.\n",
      "\n",
      "### 3.2 Strengths and Benefits\n",
      "\n",
      "- **Enhanced Embeddings:** The use of triplet loss helps capture subtle semantic differences that may be missed by conventional classification. This is particularly beneficial when dealing with nuanced tasks such as rule violation detection.\n",
      "- **Improved Generalization in Few-Shot Scenarios:** Given the scarcity of explicit rule examples, ensuring that embeddings generalize well via triplet training is an advantage (Li et al., 2021).\n",
      "- **Modular Training Phases:** The two-stage process decouples the representation learning (unsupervised or semi-supervised triplet phase) from the decision boundary learning (supervised classification phase), which can be independently tuned for better performance.\n",
      "\n",
      "### 3.3 Pitfalls and Limitations\n",
      "\n",
      "- **Triplet Mining Complexity:** Incorrect or non-informative triplet selection can compromise the benefits of triplet loss. In many cases, naive triplet mining may not capture the most challenging boundary cases, leading to a suboptimal embedding space.\n",
      "- **Catastrophic Forgetting:** The classification fine-tuning phase may inadvertently override the discriminative properties imparted during the triplet training, particularly if learning rates or regularization schemes are not well balanced.\n",
      "- **Data Scarcity:** Insufficient training examples for some rules, or an imbalanced mix of violations, may lead the model to overfit on seen rules, reducing its capacity to generalize to hidden or new rules.\n",
      "- **Computational Costs:** The two-stage training process can be computationally expensive, especially during the triplet mining stage, which can be intensive if hard negative mining strategies are employed.\n",
      "- **Overfitting to Training Rules:** With a limited set of training rules, the model may inadvertently learn rule-specific nuances (or even spurious features) rather than generalizable markers of rule violations.\n",
      "\n",
      "### 3.4 Comparative Analysis\n",
      "\n",
      "Other approaches include:\n",
      "\n",
      "- **Standard Fine-Tuning:** Directly fine-tuning DeBERTa-v3-large on classification without intermediate triplet training. This method is simpler and less computationally demanding but might struggle with generalization when rules vary.\n",
      "- **Meta-Learning / Few-Shot Approaches:** Algorithms such as MAML enable the model to quickly adapt to new rules with minimal examples. These approaches have been shown to provide better generalization in scenarios with dynamic or unseen rule structures (Finn et al., 2017).\n",
      "- **In-Context Learning and Prompt Tuning:** Providing structured prompts or instructions (e.g., RulePrompt) can guide the model to leverage its pre-trained knowledge more robustly when dealing with unseen rules. Prompt engineering can be an effective method when training data is scarce and rules are dynamic (Schick & SchÃ¼tze, 2021).\n",
      "\n",
      "While the two-stage triplet-plus-classification approach offers a structured method to initialize embeddings followed by rule-specific fine-tuning, research indicates that alternative strategies such as meta-learning or prompt-based methods may yield improved generalizability under conditions of rule diversity.\n",
      "\n",
      "## 4. Metrics and Evaluation Strategy\n",
      "\n",
      "Accurate evaluation is key to understanding how well a model generalizes to unseen rules:\n",
      "\n",
      "- **Traditional Metrics:** Metrics such as accuracy, precision, recall, and F1-score provide a baseline measure of overall performance.\n",
      "- **Advanced Metrics:** Depth F1 and rule learnability metrics evaluate performance on examples that deviate from typical training distributions. The Matthews correlation coefficient (MCC) can be valuable for imbalanced binary tasks.\n",
      "- **Interpretability Assessments:** Using SHAP, LIME, Integrated Gradients, or Class Activation Mapping (CAM) helps ensure that the decision boundaries are grounded on relevant features rather than spurious correlations.\n",
      "\n",
      "The evaluation pipeline should include a validation set with known rules and a separate test set introducing hidden rules. This two-part validation ensures that the model's robust performance on unseen rule configurations is not simply an artifact of overfitting.\n",
      "\n",
      "## 5. Alternative Approaches and Improved Workflows\n",
      "\n",
      "### 5.1 Meta-Learning and Few-Shot Strategies\n",
      "\n",
      "Meta-learning can be employed to equip the model with the ability to adapt quickly to new rules. Techniques such as Model-Agnostic Meta-Learning (MAML) train the model on a distribution of tasks so that it learns learning strategies, thereby generalizing better to unseen tasks. This approach directly addresses the challenge of having a training set with only a limited set of rules.\n",
      "\n",
      "### 5.2 Prompt-Based In-Context Learning\n",
      "\n",
      "In-context learning, where the model is given prompts that include examples or instructions at inference time, can be an effective alternative. Modern large language models have demonstrated that carefully engineered prompts can significantly improve performance on tasks with unseen rules. This approach is particularly useful if retraining is computationally prohibitive.\n",
      "\n",
      "### 5.3 Combining Contrastive and Classification Losses\n",
      "\n",
      "Rather than a strict two-stage process, a joint training strategy could be explored. For instance, integrating a supervised contrastive loss that blends the triplet objective with the classification objective can maintain robust embedding structure while simultaneously optimizing decision boundaries. This approach requires careful loss scheduling but can reduce catastrophic forgetting.\n",
      "\n",
      "### 5.4 Domain Adaptation Techniques\n",
      "\n",
      "Domain adaptation methods, especially those that aim at aligning the latent spaces of the training (source) and testing (target) rules, can improve robustness. Unsupervised adversarial domain adaptation may help the model learn domain-invariant features, reducing the performance drop on unseen rules.\n",
      "\n",
      "## 6. Conclusion and Recommendations\n",
      "\n",
      "The two-stage approach of starting with triplet loss fine-tuning followed by classification fine-tuning on DeBERTa-v3-large is a promising method for learning discriminative embeddings and refining them in the context of rule-based decision-making. However, the workflow is not without pitfalls. Challenges including triplet mining complexity, overfitting, catastrophic forgetting, and high computational costs can undermine generalizability.\n",
      "\n",
      "For practitioners, careful attention should be paid to:\n",
      "\n",
      "- **Triplet Mining Strategy:** Employ dynamic or hard-mining techniques to maximize the informative nature of triplets.\n",
      "- **Balanced Training:** Use regularization techniques such as dropout and weight decay, and consider freezing lower layers during the second fine-tuning phase to prevent catastrophic forgetting.\n",
      "- **Evaluation Protocols:** Incorporate both traditional and advanced metrics (e.g., Depth F1, MCC) to assess performance on unseen rules.\n",
      "- **Exploring Alternatives:** Consider meta-learning, prompt-based learning, or combined loss functions for a potentially more robust approach to classification when facing hidden rules.\n",
      "\n",
      "**Disclaimer:** This report is for informational purposes only and does not constitute professional, financial, or legal advice.\n",
      "\n",
      "## 7. Follow-Up Research Questions\n",
      "\n",
      "1. How might dynamic triplet mining strategies be optimized to mitigate overfitting while maximizing embedding discriminability in rule-based classification tasks?\n",
      "2. Can a joint training strategy that integrates supervised contrastive loss with classification loss outperform the sequential two-stage approach in generalization to unseen rules?\n",
      "3. What are the comparative benefits of meta-learning versus prompt-based in-context learning when adapting to dynamically evolving rule frameworks?\n",
      "4. How effective are unsupervised domain adaptation techniques in aligning the latent spaces of training and testing rules across diverse datasets?\n",
      "5. What impact does freezing various layers of DeBERTa-v3-large during classification fine-tuning have on maintaining the benefits of triplet-based embedding learning?\n",
      "\n",
      "\n",
      "\n",
      "=====FOLLOW UP QUESTIONS=====\n",
      "\n",
      "\n",
      "1. How might dynamic triplet mining strategies be optimized to mitigate overfitting while maximizing embedding discriminability in rule-based classification tasks?\n",
      "2. Can a joint training strategy that integrates supervised contrastive loss with classification loss outperform the sequential two-stage approach in generalization to unseen rules?\n",
      "3. What are the comparative benefits of meta-learning versus prompt-based in-context learning when adapting to dynamically evolving rule frameworks?\n",
      "4. How effective are unsupervised domain adaptation techniques in aligning the latent spaces of training and testing rules across diverse datasets?\n",
      "5. What impact does freezing various layers of DeBERTa-v3-large during classification fine-tuning have on maintaining the benefits of triplet-based embedding learning?\n"
     ]
    }
   ],
   "source": [
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Two-Step Fine-Tuning Approach for Rule-Based Comment Classification\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Rule-based comment classification aims to determine whether a comment violates a given rule. In practical scenarios, training data may provide explicit examples for only a subset of rules, while the test set may incorporate undisclosed or hidden rules. Such mismatches compel models to generalize beyond the explicit patterns provided during training. This report evaluates a two-stage approach using DeBERTa-v3-large: initially fine-tuning with triplet loss followed by standard classification fine-tuning. We describe what happens during this workflow, assess its effectiveness, identify pitfalls, and suggest possible improvements.\n",
    "\n",
    "## 2. Methodology and Workflow\n",
    "\n",
    "### 2.1 DeBERTa-v3-large Overview\n",
    "\n",
    "DeBERTa-v3-large is a state-of-the-art transformer-based language model that builds on its predecessor with enhancements such as ELECTRA-style pre-training and gradient-disentangled embedding sharing. With 24 layers and 304 million parameters, its architecture is designed to capture fine-grained semantic features, allowing robust performance across various NLP tasks (Microsoft, 2021). Its strong baseline performance on benchmarks like GLUE and CommonsenseQA makes it a logical choice for comment classification tasks.\n",
    "\n",
    "### 2.2 Stage One: Triplet Loss Fine-Tuning\n",
    "\n",
    "The first stage involves fine-tuning DeBERTa-v3-large using triplet loss. The triplet setup requires three samples:\n",
    "\n",
    "- **Anchor (Comment)**: the real user comment under analysis.\n",
    "- **Positive Example**: a comment that is similar in context or compliance with the given rule.\n",
    "- **Negative Example**: a comment that violates the rule or is dissimilar to the anchor.\n",
    "\n",
    "Triplet loss enforces that the distance (in the learned embedding space) between the anchor and the positive sample is smaller than that between the anchor and the negative sample by at least a set margin. This training mechanism encourages the model to learn discriminative features, mapping compliant comments closer together and distancing violative ones.\n",
    "\n",
    "**Key Benefits:**\n",
    "\n",
    "- **Discriminative Representation:** The learned embedding space is structured to reflect semantic similarities and differences.\n",
    "- **Handling Limited Labeled Data:** Triplet loss is effective in few-shot learning settings, which is valuable when positive and negative examples are limited.\n",
    "\n",
    "**Challenges:**\n",
    "\n",
    "- **Triplet Mining:** Selecting informative triplets is crucial. Poor triplet selection might result in embeddings that do not reflect the intricacies of the violation patterns.\n",
    "- **Computational Overhead:** Mining hard triplets and computing pairwise distances increases computational complexity.\n",
    "\n",
    "### 2.3 Stage Two: Rule-Based Classification Fine-Tuning\n",
    "\n",
    "Following the triplet loss phase, the model undergoes an additional fine-tuning phase for classification. The classification task uses a format that includes:\n",
    "\n",
    "- **RULE:** The rule description or trigger condition.\n",
    "- **POSITIVE EXAMPLE:** An exemplar that satisfies the rule's constraints.\n",
    "- **NEGATIVE EXAMPLE:** An exemplar that violates the rule.\n",
    "- **DECISION:** The final prediction indicating whether the comment violates the rule.\n",
    "\n",
    "This stage is conceptually similar to traditional supervised fine-tuning, but it includes structured inputs (rule and examples) as contextual adjuncts. Using the embeddings refined during the triplet training phase, the model is expected to fine-tune its decision boundary in accordance with the rule-based aspects of the classification task.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- **Contextualization:** The explicit inclusion of rule text and exemplar references helps guide the model to relate the learned embeddings with specific rule semantics.\n",
    "- **Rule Generalization:** If trained well, the model may extrapolate learned patterns to unseen rules.\n",
    "\n",
    "**Potential Pitfalls:**\n",
    "\n",
    "- **Catastrophic Forgetting:** The sequential fine-tuning may override some of the benefits yielded by the initial triplet loss stage, especially if the latter phase does not maintain the embedding distribution.\n",
    "- **Generalization to Hidden Rules:** Fine-tuning on a limited rule set might not adequately cover the variability of unseen or more complex rules.\n",
    "- **Data Scarcity and Overfitting:** Limited examples per rule (both in positive and negative instances) can lead to overfitting, where the model may learn rule-specific artifacts rather than generalizable features.\n",
    "\n",
    "## 3. In-Depth Analysis of the Workflow\n",
    "\n",
    "### 3.1 What Happens During Training:\n",
    "\n",
    "1. **Embedding Space Structuring (Triplet Training):**\n",
    "   - The model first learns to differentiate between compliant and non-compliant comments by adjusting embeddings. With triplet loss, the modelâ€™s focus is on reducing the gap between similar samples while increasing the gap against dissimilar ones.\n",
    "   \n",
    "2. **Context Incorporation (Classification Fine-Tuning):**\n",
    "   - The integration of rule, positive example, and negative example into the training input allows the model to anchor its decisions around semantic cues that are rule-specific. This step ideally refines the modelâ€™s ability to map rule descriptions onto the embedding space.\n",
    "   \n",
    "3. **Learning Decision Boundaries:**\n",
    "   - The final classification head is fine-tuned to output binary decisions based on the structured contextual input. The earlier triplet training ideally improves the robustness of these boundaries by ensuring that the underlying representation is semantically rich.\n",
    "\n",
    "### 3.2 Strengths and Benefits\n",
    "\n",
    "- **Enhanced Embeddings:** The use of triplet loss helps capture subtle semantic differences that may be missed by conventional classification. This is particularly beneficial when dealing with nuanced tasks such as rule violation detection.\n",
    "- **Improved Generalization in Few-Shot Scenarios:** Given the scarcity of explicit rule examples, ensuring that embeddings generalize well via triplet training is an advantage (Li et al., 2021).\n",
    "- **Modular Training Phases:** The two-stage process decouples the representation learning (unsupervised or semi-supervised triplet phase) from the decision boundary learning (supervised classification phase), which can be independently tuned for better performance.\n",
    "\n",
    "### 3.3 Pitfalls and Limitations\n",
    "\n",
    "- **Triplet Mining Complexity:** Incorrect or non-informative triplet selection can compromise the benefits of triplet loss. In many cases, naive triplet mining may not capture the most challenging boundary cases, leading to a suboptimal embedding space.\n",
    "- **Catastrophic Forgetting:** The classification fine-tuning phase may inadvertently override the discriminative properties imparted during the triplet training, particularly if learning rates or regularization schemes are not well balanced.\n",
    "- **Data Scarcity:** Insufficient training examples for some rules, or an imbalanced mix of violations, may lead the model to overfit on seen rules, reducing its capacity to generalize to hidden or new rules.\n",
    "- **Computational Costs:** The two-stage training process can be computationally expensive, especially during the triplet mining stage, which can be intensive if hard negative mining strategies are employed.\n",
    "- **Overfitting to Training Rules:** With a limited set of training rules, the model may inadvertently learn rule-specific nuances (or even spurious features) rather than generalizable markers of rule violations.\n",
    "\n",
    "### 3.4 Comparative Analysis\n",
    "\n",
    "Other approaches include:\n",
    "\n",
    "- **Standard Fine-Tuning:** Directly fine-tuning DeBERTa-v3-large on classification without intermediate triplet training. This method is simpler and less computationally demanding but might struggle with generalization when rules vary.\n",
    "- **Meta-Learning / Few-Shot Approaches:** Algorithms such as MAML enable the model to quickly adapt to new rules with minimal examples. These approaches have been shown to provide better generalization in scenarios with dynamic or unseen rule structures (Finn et al., 2017).\n",
    "- **In-Context Learning and Prompt Tuning:** Providing structured prompts or instructions (e.g., RulePrompt) can guide the model to leverage its pre-trained knowledge more robustly when dealing with unseen rules. Prompt engineering can be an effective method when training data is scarce and rules are dynamic (Schick & SchÃ¼tze, 2021).\n",
    "\n",
    "While the two-stage triplet-plus-classification approach offers a structured method to initialize embeddings followed by rule-specific fine-tuning, research indicates that alternative strategies such as meta-learning or prompt-based methods may yield improved generalizability under conditions of rule diversity.\n",
    "\n",
    "## 4. Metrics and Evaluation Strategy\n",
    "\n",
    "Accurate evaluation is key to understanding how well a model generalizes to unseen rules:\n",
    "\n",
    "- **Traditional Metrics:** Metrics such as accuracy, precision, recall, and F1-score provide a baseline measure of overall performance.\n",
    "- **Advanced Metrics:** Depth F1 and rule learnability metrics evaluate performance on examples that deviate from typical training distributions. The Matthews correlation coefficient (MCC) can be valuable for imbalanced binary tasks.\n",
    "- **Interpretability Assessments:** Using SHAP, LIME, Integrated Gradients, or Class Activation Mapping (CAM) helps ensure that the decision boundaries are grounded on relevant features rather than spurious correlations.\n",
    "\n",
    "The evaluation pipeline should include a validation set with known rules and a separate test set introducing hidden rules. This two-part validation ensures that the model's robust performance on unseen rule configurations is not simply an artifact of overfitting.\n",
    "\n",
    "## 5. Alternative Approaches and Improved Workflows\n",
    "\n",
    "### 5.1 Meta-Learning and Few-Shot Strategies\n",
    "\n",
    "Meta-learning can be employed to equip the model with the ability to adapt quickly to new rules. Techniques such as Model-Agnostic Meta-Learning (MAML) train the model on a distribution of tasks so that it learns learning strategies, thereby generalizing better to unseen tasks. This approach directly addresses the challenge of having a training set with only a limited set of rules.\n",
    "\n",
    "### 5.2 Prompt-Based In-Context Learning\n",
    "\n",
    "In-context learning, where the model is given prompts that include examples or instructions at inference time, can be an effective alternative. Modern large language models have demonstrated that carefully engineered prompts can significantly improve performance on tasks with unseen rules. This approach is particularly useful if retraining is computationally prohibitive.\n",
    "\n",
    "### 5.3 Combining Contrastive and Classification Losses\n",
    "\n",
    "Rather than a strict two-stage process, a joint training strategy could be explored. For instance, integrating a supervised contrastive loss that blends the triplet objective with the classification objective can maintain robust embedding structure while simultaneously optimizing decision boundaries. This approach requires careful loss scheduling but can reduce catastrophic forgetting.\n",
    "\n",
    "### 5.4 Domain Adaptation Techniques\n",
    "\n",
    "Domain adaptation methods, especially those that aim at aligning the latent spaces of the training (source) and testing (target) rules, can improve robustness. Unsupervised adversarial domain adaptation may help the model learn domain-invariant features, reducing the performance drop on unseen rules.\n",
    "\n",
    "## 6. Conclusion and Recommendations\n",
    "\n",
    "The two-stage approach of starting with triplet loss fine-tuning followed by classification fine-tuning on DeBERTa-v3-large is a promising method for learning discriminative embeddings and refining them in the context of rule-based decision-making. However, the workflow is not without pitfalls. Challenges including triplet mining complexity, overfitting, catastrophic forgetting, and high computational costs can undermine generalizability.\n",
    "\n",
    "For practitioners, careful attention should be paid to:\n",
    "\n",
    "- **Triplet Mining Strategy:** Employ dynamic or hard-mining techniques to maximize the informative nature of triplets.\n",
    "- **Balanced Training:** Use regularization techniques such as dropout and weight decay, and consider freezing lower layers during the second fine-tuning phase to prevent catastrophic forgetting.\n",
    "- **Evaluation Protocols:** Incorporate both traditional and advanced metrics (e.g., Depth F1, MCC) to assess performance on unseen rules.\n",
    "- **Exploring Alternatives:** Consider meta-learning, prompt-based learning, or combined loss functions for a potentially more robust approach to classification when facing hidden rules.\n",
    "\n",
    "**Disclaimer:** This report is for informational purposes only and does not constitute professional, financial, or legal advice.\n",
    "\n",
    "## 7. Follow-Up Research Questions\n",
    "\n",
    "1. How might dynamic triplet mining strategies be optimized to mitigate overfitting while maximizing embedding discriminability in rule-based classification tasks?\n",
    "2. Can a joint training strategy that integrates supervised contrastive loss with classification loss outperform the sequential two-stage approach in generalization to unseen rules?\n",
    "3. What are the comparative benefits of meta-learning versus prompt-based in-context learning when adapting to dynamically evolving rule frameworks?\n",
    "4. How effective are unsupervised domain adaptation techniques in aligning the latent spaces of training and testing rules across diverse datasets?\n",
    "5. What impact does freezing various layers of DeBERTa-v3-large during classification fine-tuning have on maintaining the benefits of triplet-based embedding learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report on Limitations and Considerations for Telehealth Providers of Behavioral Health Services in New York State\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The rapid expansion of telehealth services has reshaped the delivery of behavioral health care, offering both increased access and new challenges. This report explores the key limitations and considerations for telehealth providers offering behavioral health services in New York State. As telehealth becomes an increasingly vital component of mental health and substance use treatment, providers must navigate a complex landscape of federal and state regulations. This report aims to inform compliance officers and behavioral health executives about the critical requirements and pitfalls in ensuring that their telehealth practices align with both legal and regulatory obligations.\n",
    "\n",
    "## 2. Regulatory Framework Overview\n",
    "\n",
    "Telehealth services in the behavioral health domain are subject to a multifaceted regulatory framework. The federal and state laws intersect in several ways, resulting in specific obligations relating to licensure, patient consent, privacy, and reimbursement. Where federal statutes (e.g., HIPAA, 42 CFR Part 2, MHPAEA, ACA) set the baseline standards, state-specific regulations, particularly those developed by the New York State Education Departmentâ€™s Office of the Professions and the New York State Office of Mental Health (OMH), shape the operational details. These state rules address several key areas:\n",
    "\n",
    "- **Licensure and Credentialing:** Practitioners must hold valid New York licenses to deliver behavioral health services remotely, regardless of their physical location during the telehealth session (New York State Education Department, op.nysed.gov).\n",
    "- **Medicaid Reimbursement:** Providers must comply with Medicaid requirements, including registration in New York State Medicaid programs and adhering to guidelines for service documentation (New York State Department of Health, health.ny.gov).\n",
    "- **Privacy, Security, and Consent:** Telehealth interactions must meet HIPAA and NY HIPA standards for protecting patient information, including protocols for informed consent, secure data transmission, and electronic record management.\n",
    "- **Prescribing Controlled Substances:** New York and federal guidelines impose specific requirements for prescribing medications remotely through telehealth, often necessitating prior in-person evaluations under defined circumstances.\n",
    "\n",
    "## 3. Federal Considerations\n",
    "\n",
    "Federal law forms the bedrock of privacy, data security, and the appropriate use of telehealth in healthcare. Key federal considerations include:\n",
    "\n",
    "- **Health Insurance Portability and Accountability Act (HIPAA):** HIPAA sets rigorous standards for protecting patient health information and requires that telehealth platforms ensure data security and privacy. Providers must implement technical safeguards to prevent unauthorized access, especially during the electronic transmission of sensitive behavioral health data (HIPAA, 45 CFR Parts 160/164).\n",
    "\n",
    "- **42 CFR Part 2:** For substance use treatment programs, this regulation limits disclosures of patient records that contain sensitive information. Any telehealth platform used to deliver behavioral health services in areas such as substance abuse must strictly adhere to these confidentiality standards (42 CFR Part 2, SAMHSA, recent guidance).\n",
    "\n",
    "- **DEA Guidelines:** With respect to the prescription of controlled substances via telehealth, the Drug Enforcement Administration (DEA) has extended flexibilities, although New York State rules may at times impose stricter provincial parameters such as a mandatory in-person evaluation with specific exceptions (DEA/HHS, telehealth prescribing guidelines, 2025).\n",
    "\n",
    "## 4. New York State Regulatory Environment\n",
    "\n",
    "### 4.1 Licensure and Credentialing\n",
    "\n",
    "In New York, the Department of Educationâ€™s Office of the Professions is the primary licensing authority. Telehealth providers must:\n",
    "\n",
    "- Hold a valid New York State license or equivalent authorization to practice in New York. This applies irrespective of the providerâ€™s location when delivering care to patients within the state (op.nysed.gov).\n",
    "- Obtain New York-specific licensure for practitioners practicing from out-of-state. This necessitates not only holding the appropriate credentials but also complying with New Yorkâ€™s telepractice guidelines (OMH, 14 NYCRR Part 596).\n",
    "\n",
    "### 4.2 Medicaid Reimbursement and Payment Parity\n",
    "\n",
    "New York requires telehealth practitioners to be fully enrolled in the New York State Medicaid program. Compliance includes:\n",
    "\n",
    "- Registering with the Medicaid program and adhering to its billing and documentation practices, including accurately recording the patientâ€™s physical location during each session (health.ny.gov).\n",
    "- Following network adequacy and payment parity regulations. For example, regulations effective from July 2025 mandate that initial outpatient behavioral health appointments be accessible within designated business day limits. This ensures that both telehealth and in-person services are reimbursed at aligned rates, promoting access and continuity of care (governor.ny.gov, DFS press releases).\n",
    "\n",
    "### 4.3 Privacy, Informed Consent, and Confidentiality\n",
    "\n",
    "Privacy and confidentiality remain paramount in behavioral health telemedicine. In New York, providers must:\n",
    "\n",
    "- Secure informed consent either by written documentation or detailed electronic notes. The consent process must explain the benefits and risks of telehealth, the modalities used (video, audio-only, etc.), and patient rights regarding withdrawal of consent (casetext.com).\n",
    "- Ensure that both the telehealth platform and the practices around data storage comply with HIPAA, as well as New Yorkâ€™s recent legislative measures such as the NY Health Information Privacy Act (NY HIPA) (health.ny.gov; wilmerhale.com).\n",
    "- Use encrypted connections and secure servers to minimize risks of data breaches or unauthorized access during telehealth sessions.\n",
    "\n",
    "### 4.4 Controlled Substances and Prescribing\n",
    "\n",
    "New York State imposes additional constraints on the telehealth prescription of controlled substances:\n",
    "\n",
    "- Providers must conduct an in-person medical evaluation, with narrowly defined exceptions when telehealth prescribing is permissible. These exceptions include situations where the patient has had a recent in-person evaluation, emergency cases, or temporary coverage scenarios (NYSDOH, nixonpeabody.com).\n",
    "- Practitioners prescribing medications must adhere to guidelines including checking the Prescription Monitoring Program (PMP) Registry and complying with DEA regulations. Telehealth prescribing must mirror the quality and safety standards of in-person care.\n",
    "\n",
    "### 4.5 Supervision and Training Requirements\n",
    "\n",
    "For certain professionals such as Licensed Master Social Workers (LMSWs) and Licensed Mental Health Counselors (LMHCs), supervision takes on added significance:\n",
    "\n",
    "- Supervision is required to ensure that newer practitioners meet the clinical standards of New York State. This includes specified hours of supervised practice, which can be conducted remotely under secure telehealth conditions (naswnys.org; op.nysed.gov).\n",
    "- Continuing education (CE) requirements remain in full force. Providers must actively participate in recertification activities and CE programs, including specialized training for telehealth delivery to ensure technological competence and adherence to updated standards (regs.health.ny.gov).\n",
    "\n",
    "## 5. Additional Considerations and Practical Challenges\n",
    "\n",
    "While the regulatory guidelines assist in maintaining quality and safety, several practical considerations remain for providers:\n",
    "\n",
    "### 5.1 Platform and Technical Infrastructure\n",
    "\n",
    "- Telehealth platforms must be capable of secure, encrypted communications. Providers should invest in solutions that ensure reliability, even in cases of fluctuating bandwidth, especially in rural areas where access may be limited (health.ny.gov).\n",
    "- Contingency planning is crucial. Protocols must be in place to address technical failures, which include backup arrangements and alternate communication methods to prevent service disruption.\n",
    "\n",
    "### 5.2 Jurisdictional Conflicts and Compliance Risks\n",
    "\n",
    "- There is an ongoing need to reconcile federal standards with state-specific regulations. Providers must remain alert to situations where state â€œfloor versus ceilingâ€ rules come into play. For instance, while federal law may offer certain flexibilities under the telehealth umbrella, New Yorkâ€™s requirements (such as stringent in-person evaluations for controlled substance prescribing) represent a higher standard that must be met.\n",
    "- Non-compliance can result in administrative penalties, loss of licensure, or other legal consequences. Continuous internal audits and training updates are essential measures to mitigate these risks.\n",
    "\n",
    "### 5.3 Rural Access and Network Adequacy\n",
    "\n",
    "- Recognizing the critical value of telehealth in rural settings, New York has introduced initiatives like telehealth backpacks and enhanced Medicaid network adequacy requirements to ensure patients receive timely behavioral health interventions. Providers must assess how their service delivery fits into these broader state initiatives.\n",
    "- Providers must also navigate the digital divide by ensuring that patients in low-bandwidth or underserved areas receive adequate support and have access to alternative contact methods when necessary (healthweb-back.health.ny.gov).\n",
    "\n",
    "## 6. Risk Analysis and Enforcement Trends\n",
    "\n",
    "### 6.1 Pre-emption and Conflicts between Federal and State Standards\n",
    "\n",
    "It is imperative that telehealth providers understand the balance between complying with federal laws and meeting New York Stateâ€™s enhanced regulations. Federal law typically provides a minimum floor, while New York regulations often prescribe additional or more stringent requirements. For instance, the federal telehealth flexibilities for controlled substances under the DEA may be subject to New Yorkâ€™s mandatory in-person evaluation rules unless specific exceptions apply (nixonpeabody.com).\n",
    "\n",
    "### 6.2 Common Compliance Pitfalls\n",
    "\n",
    "Compliance failures often arise from:\n",
    "\n",
    "- Inadequate documentation of informed consent and patient locations during telehealth sessions.\n",
    "- Overlooking the nuances of state-specific licensure requirements for out-of-state providers attempting to treat New York residents.\n",
    "- Insufficient technical safeguards, which could expose sensitive behavioral health information to unauthorized access or data breaches.\n",
    "\n",
    "### 6.3 Enforcement and Penalties\n",
    "\n",
    "Both state and federal authorities are vigilant in enforcing these standards. Violations may lead to significant financial penalties, revocation of licenses, or civil litigation. In light of heightened enforcement trends, regular internal audits, updated training for staff, and continual monitoring of both federal and state legislative changes are recommended.\n",
    "\n",
    "## 7. Best Practices and Recommendations\n",
    "\n",
    "For telehealth providers in New York offering behavioral health services, the following best practices are recommended:\n",
    "\n",
    "- **Ensure Full Licensure Compliance:** Verify that all practitioners hold valid New York State licensure and maintain an up-to-date registration with Medicaid programs.\n",
    "- **Strengthen Informed Consent Processes:** Develop robust informed consent protocols, including detailed patient education on telehealth risks, benefits, and alternatives, with clear documentation in the clinical record.\n",
    "- **Upgrade Technical Infrastructure:** Invest in secure, compliant telehealth platforms with redundant systems to manage connectivity issues.\n",
    "- **Implement Rigorous Training:** Facilitate continuing education for all staff focused on telehealth best practices, regulatory updates, and risk management in behavioral health.\n",
    "- **Conduct Regular Audits:** Periodically review protocols, documentation practices, and compliance measures to mitigate penalties and enhance the overall safety and reliability of telehealth services.\n",
    "\n",
    "## 8. Future Developments and Legislative Outlook\n",
    "\n",
    "The regulatory landscape governing telehealth services in New York is evolving. Future legislative initiatives may further transform licensure requirements, data privacy rules, or payment parity regulations. Stakeholders should monitor developments such as:\n",
    "\n",
    "- **Legislative Adjustments in NY HIPA:** Evolving privacy regulations may impose new obligations on how providers document and secure patient data.\n",
    "- **Expansion of Telehealth Prescribing Guidelines:** Ongoing discussions at both federal and state levels regarding the telehealth prescription of controlled substances may result in further regulatory amendments.\n",
    "- **State-Specific Innovations in Virtual Care Delivery:** New Yorkâ€™s commitment to increasing telehealth access, especially in rural areas, may lead to additional state-sponsored initiatives focused on infrastructure and reimbursement adjustments.\n",
    "\n",
    "It remains essential that behavioral health executives and compliance teams remain proactive in assessing and updating internal policies in accordance with these future trends. Collaboration with legal advisors and continuous engagement with industry associations, such as the New York State Psychological Association and the Office of Mental Health, can further enhance preparedness.\n",
    "\n",
    "## 9. Conclusion\n",
    "\n",
    "Telehealth offers transformative potential for improving access to behavioral health services in New York State. However, providers face a complex regulatory landscape that spans federal mandates and state-specific rules. Compliance with licensure standards, Medicaid requirements, privacy protocols, and controlled substance prescribing rules is critical. Additionally, awareness of technical, jurisdictional, and supervisory challenges is essential to mitigate risks and align service delivery with evolving regulatory requirements.\n",
    "\n",
    "This report is for informational purposes only and does not constitute legal advice. Providers are encouraged to consult with legal counsel or compliance experts to address their specific circumstances and ensure complete adherence to all applicable regulations.\n",
    "\n",
    "\n",
    "\n",
    "=====FOLLOW UP QUESTIONS=====\n",
    "\n",
    "\n",
    "1. How might recent legislative initiatives in New York further modify telehealth requirements for controlled substance prescribing?\n",
    "2. What specific technological standards and security protocols are recommended for ensuring HIPAA compliance in telehealth platforms?\n",
    "3. How can telehealth providers optimize their informed consent procedures to address both state and federal regulatory requirements?\n",
    "4. What are the potential legal implications and enforcement actions associated with incomplete documentation of patient location and consent in telehealth sessions?\n",
    "5. How might ongoing rural healthcare initiatives and network adequacy standards impact future Medicaid reimbursement policies for behavioral health telehealth services?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sample Report in Markdown \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Agents SDK: A Comprehensive Report\n",
    "\n",
    "*Published: October 2023*\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Core Concepts and Key Features](#core-concepts-and-key-features)\n",
    "3. [Architecture and Developer Experience](#architecture-and-developer-experience)\n",
    "4. [Comparative Analysis with Alternative Frameworks](#comparative-analysis-with-alternative-frameworks)\n",
    "5. [Integrations and Real-World Applications](#integrations-and-real-world-applications)\n",
    "6. [Troubleshooting, Observability, and Debugging](#troubleshooting-observability-and-debugging)\n",
    "7. [Community Impact and Future Directions](#community-impact-and-future-directions)\n",
    "8. [Conclusion](#conclusion)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In March 2025, OpenAI released the Agents SDK, a groundbreaking, open-source framework aimed at simplifying the development of autonomous AI agents capable of performing intricate tasks with minimal human intervention. Designed with a Python-first approach, the SDK offers a minimal set of abstractions, yet provides all the necessary components to build, debug, and optimize multi-agent workflows. The release marked a significant milestone for developers who seek to integrate large language models (LLMs) with advanced task delegation mechanisms, enabling next-generation automation in various industries.\n",
    "\n",
    "The primary goal of the OpenAI Agents SDK is to streamline the creation of agentic applications by offering core primitives such as *agents*, *handoffs*, and *guardrails*. These primitives are essential for orchestrating autonomous AI systems that perform key functions such as web search, file operations, and even actions on a computer. This report delves into the SDK's features, its operational architecture, integration capabilities, and how it compares to other frameworks in the rapidly evolving landscape of AI development tools.\n",
    "\n",
    "## Core Concepts and Key Features\n",
    "\n",
    "### Agents\n",
    "\n",
    "At the heart of the SDK are **agents**â€”intelligent entities that encapsulate a specific set of instructions and tools. Each agent is built on top of a large language model and can be customized with its own personality, domain expertise, and operational directives. For example, a \"Math Tutor\" agent could be designed to solve math problems by explaining each step clearly.\n",
    "\n",
    "**Key elements of an agent include:**\n",
    "\n",
    "- **Instructions:** Specific guidelines that shape the agent's responses and behavior in the context of its designated role.\n",
    "- **Tools:** Predefined or dynamically integrated tools that the agent can leverage to access external resources (e.g., web search or file search functionalities).\n",
    "\n",
    "### Handoffs\n",
    "\n",
    "A unique feature introduced by the SDK is the concept of **handoffs**. Handoffs allow agents to delegate tasks to one another based on expertise and contextual needs. This orchestration paves the way for sophisticated workflows where multiple agents work in tandem, each contributing its specialized capabilities to complete a complex task.\n",
    "\n",
    "### Guardrails\n",
    "\n",
    "Safety and reliability remain a cornerstone in AI development, and the SDK introduces **guardrails** as a means of controlling input and output validation. Guardrails help ensure that agents operate within defined safety parameters, preventing unintended actions and mitigating risks associated with autonomous decision-making.\n",
    "\n",
    "### Built-in Debugging and Observability\n",
    "\n",
    "The development process is further enhanced by built-in **tracing and visualization tools**. These tools offer real-time insights into agent interactions, tool invocations, and decision-making pathways, thereby making debugging and optimization more accessible and systematic. The tracing functionality is a vital feature for developers looking to fine-tune agent performance in production environments.\n",
    "\n",
    "## Architecture and Developer Experience\n",
    "\n",
    "### Python-First Approach\n",
    "\n",
    "The SDK is inherently Python-based, making it highly accessible to the vast community of Python developers. By leveraging existing language features without introducing excessive abstractions, the SDK provides both simplicity and power. The installation is straightforward:\n",
    "\n",
    "```bash\n",
    "mkdir my_project\n",
    "cd my_project\n",
    "python -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip install openai-agents\n",
    "```\n",
    "\n",
    "Once installed, developers can create and configure agents with minimal boilerplate code. The emphasis on a minimal learning curve has been a significant point of praise among early adopters.\n",
    "\n",
    "### Developer Tools and Tutorials\n",
    "\n",
    "In addition to comprehensive official documentation available on OpenAIâ€™s GitHub pages, the community has contributed numerous tutorials and code examples. Video tutorials by experts such as Sam Witteveen and James Briggs provide hands-on demonstrations, ranging from simple agent creation to more sophisticated scenarios involving parallel execution and advanced tool integrations.\n",
    "\n",
    "### Use of Python's Ecosystem\n",
    "\n",
    "The integration with Pythonâ€™s ecosystem means that developers can immediately apply a range of established libraries and frameworks. For instance, utilizing Pydantic for input validation in guardrails or leveraging visualization libraries to display agent workflows are examples of how the SDK embraces the strengths of Python.\n",
    "\n",
    "## Comparative Analysis with Alternative Frameworks\n",
    "\n",
    "While the OpenAI Agents SDK has received acclaim for its simplicity and robust integration with OpenAIâ€™s ecosystem, other frameworks such as LangGraph, CrewAI, and AutoGen have emerged as viable alternatives. Hereâ€™s how they compare:\n",
    "\n",
    "- **LangGraph:** Known for its graph-based architecture, LangGraph is ideal for handling complex and cyclical workflows that require sophisticated state management. However, it comes with a steeper learning curve, making it less accessible for projects that require quick prototyping.\n",
    "\n",
    "- **CrewAI:** Emphasizing a role-based multi-agent system, CrewAI excels in scenarios where collaboration among agents is critical. Its design promotes clear segregation of duties among different agents, which can be beneficial in customer service or large-scale business automation.\n",
    "\n",
    "- **AutoGen:** This framework supports flexible conversation patterns and diverse agent interactions, particularly useful in applications where adaptive dialogue is essential. Nevertheless, AutoGen may introduce additional overhead when managing state and coordinating multiple agents.\n",
    "\n",
    "In contrast, the OpenAI Agents SDK strikes an effective balance by offering a lightweight yet powerful toolset geared towards production readiness. Its strengths lie in its minimal abstractions, ease of integration with various tools (like web search and file search), and built-in observability features that are crucial for debugging and tracing agent interactions.\n",
    "\n",
    "## Integrations and Real-World Applications\n",
    "\n",
    "### Diverse Integrations\n",
    "\n",
    "The real power of the OpenAI Agents SDK surfaces when it is integrated with other systems and platforms. Notable integrations include:\n",
    "\n",
    "- **Box Integration:** Enhancing enterprise content management, Box has adopted the SDK to enable secure AI-powered data processing. This integration allows agents to reliably access and interpret proprietary data.\n",
    "\n",
    "- **Coinbase AgentKit:** With financial capabilities in mind, Coinbase introduced AgentKit, leveraging the SDK to incorporate financial operations and risk analysis directly into AI agents.\n",
    "\n",
    "- **Milvus and Ollama:** These integrations allow the SDK to handle high-performance data queries and run agents on local infrastructure respectively, ensuring both speed and privacy.\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "The versatility of the SDK lends itself to a multitude of applications:\n",
    "\n",
    "- **Customer Support:** Automated agents can be built to handle customer inquiries, providing faster and more accurate responses while reducing workload on human agents.\n",
    "\n",
    "- **Content Generation:** In marketing and media, agents can generate high-quality articles, detailed reports, and even code reviews with built-in content guidelines.\n",
    "\n",
    "- **Financial Analysis:** Specialized agents capable of real-time data fetching and market analysis can generate actionable insights for investors and analysts.\n",
    "\n",
    "- **Health and Wellness:** Custom agents can handle tasks such as appointment scheduling, patient record management, and even provide personalized fitness and dietary recommendations.\n",
    "\n",
    "- **Educational Tools:** Intelligent tutoring agents can assist students by providing personalized learning experiences and instant feedback on assignments.\n",
    "\n",
    "These applications underscore the SDKâ€™s transformative potential across various industries, driving the trend towards increased automation and efficiency.\n",
    "\n",
    "## Troubleshooting, Observability, and Debugging\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "As with any cutting-edge technology, developers working with the OpenAI Agents SDK have encountered challenges:\n",
    "\n",
    "- **API Key Management:** Authentication errors due to missing or invalid API keys are common. The solution involves ensuring that the `OPENAI_API_KEY` environment variable is correctly set or programmatically configured using OpenAIâ€™s helper functions.\n",
    "\n",
    "- **Rate Limitations:** Rate limits, an intrinsic challenge with API-based services, require developers to monitor dashboard usage and implement retry strategies with exponential backoff.\n",
    "\n",
    "- **Response Delays:** Network latency and high server loads can result in unexpected delays. Developers are advised to check network settings, adhere to best practices in setting request timeouts, and monitor OpenAIâ€™s service status.\n",
    "\n",
    "### Built-In Tracing Capabilities\n",
    "\n",
    "The SDK provides robust tracing tools that log agent inputs, outputs, tool interactions, and error messages. This level of observability is crucial for debugging complex workflows and allows developers to visualize the agentâ€™s decision-making process in real time. By configuring a `TracingConfig` object, developers can capture detailed insights and identify performance bottlenecks.\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- **Prompt Engineering:** Refine prompts to reduce ambiguity and minimize unexpected outputs.\n",
    "- **Layered Validation:** Use guardrails extensively to ensure inputs and outputs are verified at multiple layers.\n",
    "- **Modular Design:** Break complex tasks into smaller, more manageable components using handoffs to delegate tasks appropriately.\n",
    "\n",
    "## Community Impact and Future Directions\n",
    "\n",
    "### Developer and Enterprise Adoption\n",
    "\n",
    "The release of the OpenAI Agents SDK has been met with enthusiasm within the developer community. Its ease of use, combined with comprehensive documentation and community-driven resources (such as tutorials on Class Central and DataCamp), has accelerated its adoption across educational, enterprise, and research sectors.\n",
    "\n",
    "Several leading organizations, including Box and Coinbase, have integrated the SDK into their workflows, demonstrating its capability to drive real-world business solutions. The open-source nature of the SDK, licensed under the MIT License, further encourages widespread industrial collaboration and innovation.\n",
    "\n",
    "### Future Prospects\n",
    "\n",
    "Looking forward, OpenAI plans to extend the SDKâ€™s support beyond Python, potentially embracing other programming languages like JavaScript. Additionally, future updates are anticipated to expand tool integrations, further enhance safety mechanisms, and streamline the development of multi-agent ecosystems. Planned deprecations of older APIs, such as the Assistants API in favor of the more unified Responses API, underline the SDKâ€™s evolving roadmap aimed at future-proofing agentic applications.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The OpenAI Agents SDK represents a significant step forward in the field of AI development. Its lightweight, Python-first framework facilitates the creation of autonomous agents that can handle a wide array of tasksâ€”from simple inquiries to complex multi-agent systems. The SDKâ€™s robust integration capabilities, combined with its focus on safety and observability, make it an ideal choice for both developers and enterprises seeking to build reliable, scalable agentic applications.\n",
    "\n",
    "In summary, the SDK not only lowers the barrier to entry for developing sophisticated AI applications but also sets the stage for further innovations as the ecosystem evolves. It is poised to become a standard toolkit for the next generation of AI-driven technologies, empowering users across sectors to achieve greater efficiency and creativity in task automation.\n",
    "\n",
    "---\n",
    "\n",
    "*For further reading, developers are encouraged to visit the official OpenAI documentation, join the community forums, and explore real-world use cases to deepen their understanding of this transformative tool.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
